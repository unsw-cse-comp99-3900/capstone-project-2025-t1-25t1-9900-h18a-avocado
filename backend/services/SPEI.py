import xarray as xr
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

from scipy.stats import fisk, norm

# å‡è®¾ä½ å·²æœ‰çš„å·¥å…·å‡½æ•°
from utils.readNcFiles import load_data
from utils.NRM import get_shapefile_path, extract_regions_from_shapefile


def remove_duplicate_times(ds: xr.Dataset) -> xr.Dataset:
    """
    ç”¨ groupby("time").first() çš„æ–¹å¼å»é™¤é‡å¤æ—¶é—´ç‚¹ï¼Œåªä¿ç•™æ¯ä¸ª time çš„ç¬¬ä¸€æ¡æ•°æ®ã€‚
    ä¸ä½¿ç”¨ sortby("time")ï¼Œä»è€Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨ã€‚
    """
    # å»ºè®®å…ˆå¯¹ ds åšä¸€å®šçš„ chunkï¼Œè®© groupby èšåˆæ—¶èƒ½åˆ†å—å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å…¥å†…å­˜
    ds = ds.chunk({"time": 10, "lat": 100, "lon": 100})
    # å¯¹åŒä¸€ time çš„è®°å½•ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ¡ç›®
    ds_unique = ds.groupby("time").first()
    return ds_unique


def convert_cftime_to_datetime64(ds: xr.Dataset) -> xr.Dataset:
    """
    å¦‚æœæ—¶åºä¸æ˜¯æ ‡å‡†çš„ np.datetime64 ç±»å‹ï¼Œåˆ™å°è¯•è½¬æ¢ä¸ºæ ‡å‡†æ—¶é—´ã€‚
    """
    if not isinstance(ds.indexes["time"], pd.DatetimeIndex):
        try:
            time_converted = ds.indexes["time"].to_datetimeindex()
            ds["time"] = time_converted
        except Exception as e:
            print("[âŒ] æ—¶é—´è½¬æ¢å¤±è´¥ï¼ŒåŸå› ï¼š", e)
    return ds


def extract_precipitation_evaporation(ds_pr: xr.Dataset, ds_evap: xr.Dataset) -> xr.DataArray:
    """
    è®¡ç®—æ°´åˆ†å¹³è¡¡ wb = pr - evspsblï¼Œå•ä½ mm/dayã€‚
    ä¿ç•™ Dask æƒ°æ€§è®¡ç®—ï¼Œä¸è°ƒç”¨ .load()ã€‚
    """
    # å¦‚æœæ—¶é—´ä¸æ˜¯ np.datetime64 ç±»å‹ï¼Œåˆ™å…ˆè½¬æ¢
    if not np.issubdtype(ds_pr.time.dtype, np.datetime64):
        ds_pr = ds_pr.convert_calendar("standard", use_cftime=False)
        ds_evap = ds_evap.convert_calendar("standard", use_cftime=False)

    # ç»Ÿä¸€æ—¶é—´æˆ³åˆ° 00:00:00
    ds_pr["time"] = pd.to_datetime(ds_pr.time.values).normalize()
    ds_evap["time"] = pd.to_datetime(ds_evap.time.values).normalize()

    # è½¬æ¢å•ä½ï¼šä» kg/(m^2Â·s) åˆ° mm/day
    pr = ds_pr["pr"] * 86400
    pr.attrs["units"] = "mm/day"

    evspsbl = ds_evap["evspsbl"] * 86400
    evspsbl.attrs["units"] = "mm/day"

    wb = pr - evspsbl
    wb.name = "wb"
    wb.attrs["units"] = "mm/day"

    return wb


def compute_spei_loglogistic(wb_array: np.ndarray) -> np.ndarray:
    """
    ç”¨å¯¹æ•°å¯¹æ•°ï¼ˆFiskï¼‰åˆ†å¸ƒåˆ†åˆ«æ‹Ÿåˆæ­£å€¼å’Œè´Ÿå€¼çš„æ°´åˆ†å¹³è¡¡æ•°æ®ï¼Œå¾—åˆ° SPEIã€‚
    wb_array: 1D numpy æ•°ç»„ï¼ˆå¯åŒ…å«æ­£ã€è´Ÿå€¼ï¼‰
    è¿”å›åŒé•¿åº¦çš„ SPEI æ ‡å‡†åŒ–å€¼ã€‚
    """
    wb = wb_array.flatten()
    pos_mask = wb >= 0
    neg_mask = wb < 0

    if np.sum(pos_mask) < 5 or np.sum(neg_mask) < 5:
        raise ValueError("æ­£å€¼æˆ–è´Ÿå€¼åŒºé—´æ ·æœ¬é‡è¿‡å°‘ï¼Œæ— æ³•åˆ†åˆ«æ‹Ÿåˆå¯¹æ•°å¯¹æ•°åˆ†å¸ƒã€‚")

    # æ‹Ÿåˆæ­£å€¼éƒ¨åˆ†
    pos_params = fisk.fit(wb[pos_mask], floc=0)
    F_pos = fisk.cdf(wb[pos_mask], *pos_params)
    spei_pos = norm.ppf(F_pos)

    # æ‹Ÿåˆè´Ÿå€¼éƒ¨åˆ†ï¼ˆå¯¹ç»å¯¹å€¼åšåˆ†å¸ƒï¼‰
    neg_params = fisk.fit(-wb[neg_mask], floc=0)
    F_neg = 1.0 - fisk.cdf(-wb[neg_mask], *neg_params)
    spei_neg = norm.ppf(F_neg)

    # åˆå¹¶
    spei = np.empty_like(wb)
    spei[pos_mask] = spei_pos
    spei[neg_mask] = spei_neg

    return spei


def compute_spei_for_region(
    wb_region: xr.DataArray,
    model_family: str,
    scenario: str,
    model_name: str,
    region_id: int,
    region_name: str,
    cal_start: str = "1976-01-01",
    cal_end: str = "2005-12-31",
) -> pd.DataFrame:
    """
    å¯¹ç‰¹å®šåŒºåŸŸçš„ wb æ•°æ®è®¡ç®— SPEIï¼Œè¿”å› pd.DataFrameã€‚
    """
    try:
        # æ—¶é—´æ’åºï¼ˆè‹¥éƒ½å·²æ— é‡å¤ï¼Œä¸ä¼šå å¤ªå¤§å†…å­˜ï¼‰
        wb_region = wb_region.sortby("time")
        # æœˆåº¦èšåˆ
        wb_region = wb_region.resample(time="MS").sum()

        # æ£€æŸ¥æ—¶é—´ç‚¹æ•°é‡
        if wb_region.time.size < 3:
            print(f"[âš ï¸] åŒºåŸŸ {region_id} - {region_name}ï¼šæ—¶é—´ç‚¹ä¸è¶³ 3ï¼Œè·³è¿‡è®¡ç®—ã€‚")
            return pd.DataFrame(
                columns=["time","SPEI","region_id","region_name","model_family","scenario","model_name"]
            )
        wb_region = wb_region.coarsen(lat=2, lon=2, boundary="trim").mean()#é™ä½é‡‡æ ·
        # åŒºåŸŸå¹³å‡
        wb_1d = wb_region.mean(dim=["lat","lon"], skipna=True)

        # è®¡ç®—SPEI
        spei_values = compute_spei_loglogistic(wb_1d.values)
        spei_da = xr.DataArray(spei_values, coords=[wb_1d.time], dims=["time"])
        df = spei_da.to_dataframe(name="SPEI").reset_index()

        # æ ¼å¼åŒ–æ—¶é—´
        df["time"] = pd.to_datetime(df["time"], errors="coerce").dt.strftime("%Y-%m")

        # å…ƒä¿¡æ¯
        df["region_id"] = region_id
        df["region_name"] = region_name
        df["model_family"] = model_family
        df["scenario"] = scenario
        df["model_name"] = model_name

        return df
    except Exception as e:
        print(f"[âŒ] Failed to compute SPEI for region {region_id} - {region_name}, {type(e).__name__}: {e}")
        return pd.DataFrame(
            columns=["time","SPEI","region_id","region_name","model_family","scenario","model_name"]
        )


def export_all_regions_spei_to_csv(
    region_dict: dict,
    model_family: str,
    scenario: str,
    model_name: str,
    variable: str,
    cal_start: str,
    cal_end: str,
):
    """
    å¹¶è¡Œåœ°ä¸ºæ‰€æœ‰åŒºåŸŸè®¡ç®— SPEIï¼Œå¹¶å¯¼å‡ºæ±‡æ€» CSVã€‚
    """
    all_spei_dfs = []
    print(f"ğŸŒ Starting SPEI computation for all regions (parallel)... [model={model_name}]")

    for region_id, da in region_dict.items():
        da.attrs.setdefault("units", "mm/day")

    # è¿‡æ»¤ç©ºåŒºåŸŸ
    region_dict = {
        region_id: da for region_id, da in region_dict.items()
        if da.sizes.get("lat", 0) > 0 and da.sizes.get("lon", 0) > 0
    }
    print(f"ğŸš€ Filtered and retained {len(region_dict)} non-empty regions.")

    def compute_single(region_id, data_array):
        region_name = f"Region_{region_id}"
        print(f"ğŸ“ [START] {region_id} - {region_name}")
        return compute_spei_for_region(
            wb_region=data_array,
            model_family=model_family,
            scenario=scenario,
            model_name=model_name,
            region_id=region_id,
            region_name=region_name,
            cal_start=cal_start,
            cal_end=cal_end,
        )

    with ThreadPoolExecutor(max_workers=1) as executor:
        futures = {
            executor.submit(compute_single, region_id, data_array): region_id
            for region_id, data_array in region_dict.items()
        }
        for future in as_completed(futures):
            try:
                result = future.result()
                all_spei_dfs.append(result)
            except Exception as e:
                rid = futures[future]
                print(f"[âŒ] Exception in region {rid}: {e}")

    if all_spei_dfs:
        all_spei_df = pd.concat(all_spei_dfs, ignore_index=True)
    else:
        all_spei_df = pd.DataFrame(
            columns=["time","SPEI","region_id","region_name","model_family","scenario","model_name"]
        )

    output_path = f"all_regions_spei_{model_family}_{scenario}_{variable}_{model_name}.csv"
    all_spei_df.to_csv(output_path, index=False)
    print(f"[âœ…] All region SPEI results saved to: {output_path}")
    print(f"[ğŸ“Š] Computed SPEI for {all_spei_df['region_id'].nunique()} out of {len(region_dict)} regions.")


def compute_spei(
    model_family: str,
    scenario: str,
    model_name: str,
    variable: str,
    cal_start: str = "1976-01-01",
    cal_end: str = "2005-12-31",
):
    """
    ä¸»å‡½æ•°ï¼šè¯»å– pr & evspsbl, åˆå¹¶ã€å»é‡ã€è®¡ç®—æ°´åˆ†å¹³è¡¡å¹¶åšSPEIè¾“å‡ºã€‚
    """
    print(f"ğŸ“¦ Loading {model_family} data for model={model_name}, scenario={scenario}...")

    # è¯»å…¥ historical + scenario
    ds_var_hist = load_data(model_family, "historical", variable, model_name)
    ds_var_fut = load_data(model_family, scenario, variable, model_name)

    ds_evap_hist = load_data(model_family, "historical", "evspsbl", model_name)
    ds_evap_fut = load_data(model_family, scenario, "evspsbl", model_name)

    print("Starting to merge past and future data...")
    ds_var = xr.concat([ds_var_hist, ds_var_fut], dim="time")
    ds_evap = xr.concat([ds_evap_hist, ds_evap_fut], dim="time")
    print("Merge data successfully.")

    # ============ã€å…³é”®ï¼šç”¨ groupby("time").first() å»é‡ã€‘============
    ds_var = remove_duplicate_times(ds_var)
    ds_evap = remove_duplicate_times(ds_evap)
    # å¦‚æœä»æƒ³ä¿è¯æ—¶é—´é¡ºåºï¼Œå¯å† ds_var = ds_var.sortby("time")
    # ä½†å¯¹å¤§æ•°æ®å¯èƒ½å¢åŠ å†…å­˜å ç”¨
    # ========================================================

    ds_var = convert_cftime_to_datetime64(ds_var)
    ds_evap = convert_cftime_to_datetime64(ds_evap)

    print("ğŸ§® Computing water balance (pr - evspsbl)...")
    wb = extract_precipitation_evaporation(ds_var, ds_evap)

    print("ğŸ—ºï¸  Splitting by NRM regions...")
    shapefile_path = get_shapefile_path()
    region_dict = extract_regions_from_shapefile(shapefile_path, wb)

    export_all_regions_spei_to_csv(
        region_dict,
        model_family=model_family,
        scenario=scenario,
        model_name=model_name,
        variable=variable,
        cal_start=cal_start,
        cal_end=cal_end,
    )


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šé’ˆå¯¹ CMIP6, ssp370 ä¸‹è‹¥å¹²æ¨¡å‹åš SPEI
    # CMIP5_models = ['CCCma-CanESM2', 'NCC-NorESM1-M', 'CSIRO-BOM-ACCESS1-0', 'MIROC-MIROC5', 'NOAA-GFDL-GFDL-ESM2M']
    # CMIP6_models = ['ACCESS-CM2', 'ACCESS-ESM1-5', 'CESM2', 'CNRM-ESM2-1', 'CMCC-ESM2']

    # for model_name in CMIP6_models:
    #     compute_spei("CMIP6", "ssp126", model_name=model_name, variable="pr",
    #                 cal_start="1976-01-01", cal_end="2005-12-31")
    #     compute_spei("CMIP6", "ssp370", model_name=model_name, variable="pr",
    #                 cal_start="1976-01-01", cal_end="2005-12-31")

    # for model_name in CMIP5_models:
    #     compute_spei("CMIP5", "rcp45", model_name=model_name, variable="pr",
    #                 cal_start="1976-01-01", cal_end="2005-12-31")
    #     compute_spei("CMIP5", "rcp85", model_name=model_name, variable="pr",
    #                 cal_start="1976-01-01", cal_end="2005-12-31")

    model = "CCCma-CanESM2"
    # 1976â€“2005 ä¸ºåŸºå‡†æœŸ
    start, end = "1976-01-01", "2005-12-31"


    compute_spei(
        model_family="CMIP5",
        scenario="rcp45",
        model_name=model,
        variable="pr",
        cal_start=start,
        cal_end=end,
    )


    # compute_spei(
    #     model_family="CMIP5",
    #     scenario="rcp85",
    #     model_name=model,
    #     variable="pr",
    #     cal_start=start,
    #     cal_end=end,
    # )

